RNN和RNNCell层的区别在于前者能够处理整个序列，而后者一次只能处理序列中一个时间点的数据，
前者封装更完备易于使用，后者更具灵活性。RNN层可以通过组合调用RNNCell来实现。

2.3

1. RNN

import torch as t
from torch import nn
from torch.autograd import Variable as V

t.manual_seed(1000)

# 输入 batch_size=3,序列长度都为2，序列中每个元素占4维

input=V(t.randn(2,3,4))
print(input)

# lstm 输入向量4维，3个隐藏元，1层

lstm=nn.LSTM(4,3,1)

# 初始状态：1层，batch_size=3,3个隐层元

h0=V(t.randn(1,3,3))
c0=V(t.randn(1,3,3))

# error 多传一个参数，应该是2-3个，而不是4个，默认有self
# out,(hn,cn)=lstm(input,h0,c0)
out,(hn,cn)=lstm(input,(h0,c0))
print(out)
print("hn=\n",hn)
print("cn=\n",cn)

out,hn=lstm(input,(h0,c0))
print(out)
print("hn=\n",hn)




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[[-0.5306, -1.1300, -0.6734, -0.7669],
         [-0.7029,  0.9896, -0.4482,  0.8927],
         [-0.6043,  1.0726,  1.0481,  1.0527]],

        [[-0.6424, -1.2234, -1.0794, -0.6037],
         [-0.7926, -0.1414, -1.0225, -0.0482],
         [ 0.6610, -0.8908,  1.4793, -0.3934]]])
tensor([[[-0.3610, -0.1643,  0.1631],
         [-0.0613, -0.4937, -0.1642],
         [ 0.5080, -0.4175,  0.2502]],

        [[-0.0703, -0.0393, -0.0429],
         [ 0.2085, -0.3005, -0.2686],
         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>)
hn=
 tensor([[[-0.0703, -0.0393, -0.0429],
         [ 0.2085, -0.3005, -0.2686],
         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>)
cn=
 tensor([[[-0.1092, -0.0645, -0.0755],
         [ 0.3125, -0.6556, -0.4899],
         [ 0.3707, -0.8322,  1.0514]]], grad_fn=<StackBackward>)
tensor([[[-0.3610, -0.1643,  0.1631],
         [-0.0613, -0.4937, -0.1642],
         [ 0.5080, -0.4175,  0.2502]],

        [[-0.0703, -0.0393, -0.0429],
         [ 0.2085, -0.3005, -0.2686],
         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>)
hn=
 (tensor([[[-0.0703, -0.0393, -0.0429],
         [ 0.2085, -0.3005, -0.2686],
         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>), tensor([[[-0.1092, -0.0645, -0.0755],
         [ 0.3125, -0.6556, -0.4899],
         [ 0.3707, -0.8322,  1.0514]]], grad_fn=<StackBackward>))

Process finished with exit code 0



2. RNNCell层


from torch.autograd import Variable as V
import torch as t
from torch import nn



t.manual_seed(1000)
input=V(t.randn(2,3,4))

# 一个LSTMCell对应的层数只能是一层
lstm=nn.LSTMCell(4,3)
hx=V(t.randn(3,3))
cx=V(t.randn(3,3))
out=[]
print(input)
for i_ in input:
    hx,cx=lstm(i_,(hx,cx))
    print("i_\n",i_)
    out.append(hx)
print(t.stack(out))

# 词向量在自然语言中应用十分广泛，pytorch同样提供了Embedding层

# 有4个词，每一词用5维的向量表示
embedding=nn.Embedding(4,5)
print("embedding=\n",embedding)
print(embedding.weight)
# 可以用预训练好的词向量初始化embedding
embedding.weight.data=t.arange(0,20).view(4,5)
print("embedding.weight\n",embedding.weight)

print(embedding.weight.data)
with t.no_grad():
   input=V(t.arange(3,0,-1)).long()
   print(input)
   print(embedding.weight.data)
   output = embedding(input)
   print(output)

# print("input=",input[:])


print(output)




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[[-0.5306, -1.1300, -0.6734, -0.7669],
         [-0.7029,  0.9896, -0.4482,  0.8927],
         [-0.6043,  1.0726,  1.0481,  1.0527]],

        [[-0.6424, -1.2234, -1.0794, -0.6037],
         [-0.7926, -0.1414, -1.0225, -0.0482],
         [ 0.6610, -0.8908,  1.4793, -0.3934]]])
i_
 tensor([[-0.5306, -1.1300, -0.6734, -0.7669],
        [-0.7029,  0.9896, -0.4482,  0.8927],
        [-0.6043,  1.0726,  1.0481,  1.0527]])
i_
 tensor([[-0.6424, -1.2234, -1.0794, -0.6037],
        [-0.7926, -0.1414, -1.0225, -0.0482],
        [ 0.6610, -0.8908,  1.4793, -0.3934]])
tensor([[[-0.3610, -0.1643,  0.1631],
         [-0.0613, -0.4937, -0.1642],
         [ 0.5080, -0.4175,  0.2502]],

        [[-0.0703, -0.0393, -0.0429],
         [ 0.2085, -0.3005, -0.2686],
         [ 0.1482, -0.4728,  0.1425]]], grad_fn=<StackBackward>)
embedding=
 Embedding(4, 5)
Parameter containing:
tensor([[-0.2297, -0.7947,  0.1204,  0.6523, -0.2653],
        [-0.7661, -0.2536,  0.2986,  0.2507, -0.8575],
        [-0.2504, -0.7584,  1.5276, -0.2307,  2.2975],
        [-1.5437,  1.2492, -0.5840, -0.8269,  0.0587]], requires_grad=True)
embedding.weight
 Parameter containing:
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]], requires_grad=True)
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
tensor([3, 2, 1])
tensor([[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19]])
tensor([[15, 16, 17, 18, 19],
        [10, 11, 12, 13, 14],
        [ 5,  6,  7,  8,  9]])
tensor([[15, 16, 17, 18, 19],
        [10, 11, 12, 13, 14],
        [ 5,  6,  7,  8,  9]])

Process finished with exit code 0



2.4  损失函数



from torch import nn
from torch.autograd import Variable as V
import torch as t

t.manual_seed(1)
# # batch_size=3
#
# score=V(t.ones(3,3))
# print(score)
#
# # 三个样本属于1，0，1类，label必须是LongTensor
#
# label=V(t.Tensor([1,0,1])).long()
# print(label)
#
# # loss 与 普通的layer无差异
#
# criterion= nn.ReLU()
# loss=criterion(score)
# print(loss)




# batch_size=3

score=V(t.randn(4,3))
print(score)

# 三个样本属于1，0，1类，label必须是LongTensor

label=V(t.Tensor([0,1,0,1])).long()
print(label)

# loss 与 普通的layer无差异

criterion= nn.CrossEntropyLoss()
loss=criterion(score,label)
print(loss)




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[ 0.6614,  0.2669,  0.0617],
        [ 0.6213, -0.4519, -0.1661],
        [-1.5228,  0.3817, -1.0276],
        [-0.5631, -0.8923, -0.0583]])
tensor([0, 1, 0, 1])
tensor(1.5601)

Process finished with exit code 0






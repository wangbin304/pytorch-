

良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化可能使模型迅速崩溃。
pytorch中 nn.Module的模块参数都采取了较合理地初始化策略，一般不用我们考虑。

当然，我们也可以自定义初始化代替系统的默认初始化，当我们使用Parameter时，自定义
初始化尤为重要，因为t.Tensor()返回的是内存中的随机数，很可能会有极大值，这在实际训练网络
中会造成溢出或者梯度消失。PyTorch中nn.init模块专门为初始化设计，实现了常用的初始化策略。

如果某种初始化策略nn.init不提供，用户也可以自己直接初始化。





from torch.nn import init
import torch as t
import torch.nn as nn


class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.features=nn.Sequential(nn.Conv2d(3,6,5),
                                    nn.ReLU(),
                                    nn.MaxPool2d(2,2),
                                    nn.Conv2d(6,16,5),
                                    nn.ReLU(),
                                    nn.MaxPool2d(2,2)
                                    )
        self.classifier=nn.Sequential(
            nn.Linear(16*5*5,120),
            nn.ReLU(),
            nn.Linear(120,84),
            nn.ReLU(),
            nn.Linear(84,10)
        )
    def forward(self,x):
        x=self.features(x)
        x=x.view(-1,16*5*5)
        x=self.classifier(x)
        return x
net=Net()
input=t.rand(1,3,32,32)



linear=nn.Linear(3,4)

t.manual_seed(1)
import math
# 等价于 linear.weight.data.normal_(0,std)
print("linear.weight:",linear.weight)
print("init.xavier_normal:",init.xavier_normal(linear.weight))
print(linear.weight.size(1),linear.weight.size(0))
print("hello world!\n",1.0 * math.sqrt(2.0 / float(linear.weight.size(1)+ linear.weight.size(0))))

import math
t.manual_seed(1)

# xavier初始化的计算公式

std=math.sqrt(2)/math.sqrt(7.)
print(math.sqrt(2),math.sqrt(7.))
print(std)
print(linear.weight.data.normal_(0,std))

print(linear.named_parameters)
for name,params in linear.named_parameters():
    print(linear.named_parameters())
    if name.find("linear")!=-1:
        print(params[0]) # weight
        print(params[1]) # bias
    elif name.find("conv")!=-1:
        print("hello world 123")
        pass
    elif name.find("norm")!=-1:
        pass







/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
linear.weight: Parameter containing:
tensor([[ 0.3403, -0.4769,  0.2964],
        [ 0.0109, -0.2063,  0.0658],
        [-0.1978,  0.3152,  0.5400],
        [-0.4757,  0.1936, -0.4782]], requires_grad=True)
init.xavier_normal: Parameter containing:
tensor([[ 0.3535,  0.1427,  0.0330],
        [ 0.3321, -0.2416, -0.0888],
        [-0.8140,  0.2040, -0.5493],
        [-0.3010, -0.4769, -0.0311]], requires_grad=True)
3 4
hello world!
 0.5345224838248488
1.4142135623730951 2.6457513110645907
0.5345224838248488
tensor([[ 0.3535,  0.1427,  0.0330],
        [ 0.3321, -0.2416, -0.0888],
        [-0.8140,  0.2040, -0.5493],
        [-0.3010, -0.4769, -0.0311]])
<bound method Module.named_parameters of Linear(in_features=3, out_features=4, bias=True)>
<generator object Module.named_parameters at 0x7fe89dbc1450>
<generator object Module.named_parameters at 0x7fe89dbc1450>
/home/wangbin/anaconda3/envs/deep_learning/project/main.py:39: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  print("init.xavier_normal:",init.xavier_normal(linear.weight))

Process finished with exit code 0




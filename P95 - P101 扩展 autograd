
1. 写一个Function

   绝大多数函数都可以使用autograd实现反向求导，但是仍然有一些函数不被支持反向求导，这种情况下，
我们就需要写一个Function，实现它的前向传播和反向传播代码，Function对应于计算图中的矩形，它
接收参数，计算并返回结果。


from torch.autograd import Function
from torch.autograd import Variable as V
import torch as t

##  template
# class Mul(Function):
#     @staticmethod
#     def forward(ctx, w, x, b, x_requires_grad=True):
#         ctx.x_requires_grad = x_requires_grad
#         ctx.save_for_backward(w, x)
#         output = w * x + b
#         return output
#
#     @staticmethod
#     def backward(ctx, grad_output):
#         w, x = ctx.saved_output * ctx
#         grad_w = grad_ouput * x
#         if ctx.x_requires_grad:
#             grad_x=grad_output*w
#         else:
#             grad_x=None
#         grad_b=grad_output*1
#         return grad_w,grad_x,grad_b,None


class MultiplyAdd(Function):
    @ staticmethod
    def forward(ctx,w,x,b):
        print("type in forward",type(x))
        ctx.save_for_backward(w,x)
        output=w*x+b
        return output

    @staticmethod
    def backward(ctx,grad_output):
        w,x=ctx.saved_variables
        print("type in backward",type(x))
        grad_w=grad_output*x
        grad_x=grad_output*w
        grad_b=grad_output*1
        return grad_w,grad_x,grad_b

x=V(t.ones(1))
w=V(t.rand(1),requires_grad=True)
b=V(t.rand(1),requires_grad=True)
print("开始前向传播")
z=MultiplyAdd.apply(w,x,b)
print("开始反向传播")
z.backward()
print(x.grad,w.grad,b.grad)




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
开始前向传播
type in forward <class 'torch.Tensor'>
开始反向传播
type in backward <class 'torch.Tensor'>
None tensor([1.]) tensor([1.])
/home/wangbin/anaconda3/envs/deep_learning/project/main.py:36: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'
  w,x=ctx.saved_variables

Process finished with exit code 0




2. 二阶导数求法

from __future__ import print_function
from torch.autograd import Variable as V
import torch as t

x = V(t.Tensor([5]), requires_grad=True)
y = x ** 2
grad_x = t.autograd.grad(y, x, create_graph=True)
# dy/dx=2*x
print(grad_x)

# 二阶导数
grad_grad_x=t.autograd.grad(grad_x[0],x)
print(grad_grad_x)

print(grad_x)



/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
(tensor([10.], grad_fn=<MulBackward0>),)
(tensor([2.]),)
(tensor([10.], grad_fn=<MulBackward0>),)

Process finished with exit code 0



3. 使用Function 实现 Sigmoid Function

显然，f_sigmoid比单纯利用autograd加减和乘方操作实现的函数快不少，因为f_sigmoid的backward优化了反向传播
的过程。另外，可以看出系统实现的builtin接口（t.sigmoid）更快。

from __future__ import print_function
from torch.autograd import Variable as V
from torch.autograd import Function
import torch as t

# ctx 是 context的缩写
class Sigmoid(Function):
    @ staticmethod
    def forward(ctx,x):
        output=1/(1+t.exp(-x))
        ctx.save_for_backward(output)
        return output
    @staticmethod
    def backward(ctx,grad_output):
        output, =ctx.saved_tensors
        grad_x=output*(1-output)*grad_output
        return grad_x

test_input=V(t.randn(3,4).double(),requires_grad=True)
print(t.autograd.gradcheck(Sigmoid.apply,(test_input,),eps=1e-3))


def f_sigmoid(x):
    y=Sigmoid.apply(x)
    y.backward(t.ones(x.size()))

def f_naive(x):
    y=1/(1+t.exp(-x))
    y.backward(t.ones(x.size()))

def f_th(x):
    y=t.sigmoid(x)
    y.backward(t.ones(x.size()))

x=V(t.randn(100,100),requires_grad=True)
%timeit -n 100 f_sigmoid(x)
%timeit -n 100 f_naive(x)
%timeit -n 100 f_th(x)



True
106 µs ± 1.67 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
112 µs ± 3.36 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
45.6 µs ± 11.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


4. 用Variable实现线性回归

使用 autograd/Variable实现线性回归

import torch as t
from torch.autograd import Variable as V
from matplotlib import pyplot as plt

from IPython import display


t.manual_seed(1000)

def get_fake_data(batch_size=8):
    # “产生随机数据： y=x*2+3   加上一些噪声”
    x = t.rand(batch_size, 1) * 20
    y = x * 2 + (1 + t.randn(batch_size, 1)) * 3
    return x, y



# x, y = get_fake_data()
# plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())
# plt.show()


w=V(t.rand(1,1),requires_grad=True)
b=V(t.zeros(1,1),requires_grad=True)

lr=0.001
for ii in range(8000):
    x,y=get_fake_data()
    x,y=V(x),V(y)

    # forward  计算 loss
    y_pred=x.mm(w)+b.expand_as(y)
    loss=0.5*(y_pred-y)**2
    loss=loss.sum()

    # backward: 自动计算梯度
    loss.backward()

    # 更新参数
    w.data.sub_(lr*w.grad.data)
    b.data.sub_(lr*b.grad.data)

    # 梯度清零
    w.grad.data.zero_()
    b.grad.data.zero_()

    if ii%1000==0:
        #画图
        display.clear_output(wait=True)
        x=t.arange(0.,20).view(-1,1)
        y=x.mm(w.data)+b.data.expand_as(x)
        plt.plot(x.numpy(),y.numpy())  # predicted

        x2,y2=get_fake_data(batch_size=20)
        plt.scatter(x2.numpy(),y2.numpy()) # true data

        plt.xlim(0,20)
        plt.ylim(0,41)
        plt.show()
        plt.pause(0.5)

    print(w.data.squeeze(),b.data.squeeze())





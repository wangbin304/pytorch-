

1. nn.Module

torch.nn 是专门为深度学习设计的模块，核心数据结构是Module，它是一个抽象的概念，
既可以表示神经网络中的某个层(layer),也可以表示一个包含很多层的神经网络。
在实际应用中，最常见的做法是继承nn.Module，撰写自己的网络/层。

#  Module实现的全连接层，比利用Function实现的更简单，
#  因为它不需要写反向传播函数

import torch as t
from torch import nn
from torch.autograd import Variable as V

t.manual_seed(1000)


# 1. 自定义Linear 必须继承nn.Module
#  并且在构造函数中需要调用nn.Module的构造函数
#   super(Linear,self).__init__()
#   or   nn.Module.__init__(self)

# 2. 在构造函数__init__中必须自己定义可学习的参数，
#   并封装成Parameter(特殊 Variable) ，默认需要求导。

# 3. forward 前向传播，输入可以是多个Variable

# 4. 无需写反向传播函数，nn.Module利用autograd自动实现反向传播

# 5. 将layer可以看成数学概念中的函数，调用layer(input)即
#   可以得到对应结果，等价 layers.__call__(input)
#   在 __call__函数中，主要调用的是layer.forward(x)。

# 6. Module中的科学系参数通过 named_parameters()
#   或者parameters()返回迭代器，前者给parameter附上名字


class Linear(nn.Module):  # 继承 nn.Module
    def __init__(self, in_features, out_features):
        # 等价于 nn.Module.__init__(self)
        super(Linear, self).__init__()
        self.w = nn.Parameter(t.randn(in_features, out_features))
        print(self.w.shape)
        self.b = nn.Parameter(t.randn(out_features))
        print(self.b.shape)
        # # error: cannot resize variables that require grad
        # print(self.b.shape)
        # self.b.resize_(2,3)
        # print(self.b.size())

    def forward(self, x):
        x = x.mm(self.w)
        # print(self.b.expand_as(x).shape)
        # print(x.shape)
        return x + self.b.expand_as(x)


layer = Linear(4, 3)
input = V(t.randn(2, 4))
output = layer(input)
print(output)

#  named_parameters 参数有名字
for name, parameter in layer.named_parameters():
    print(name, parameter)  # w and b
#  parameters 参数没名字
for parameter in layer.parameters():
    print(parameter)  # w and b


class Perceptron(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        nn.Module.__init__(self)
        # super(Linear, self).__init__()
        # 此处的Linear是前面自定义的全连接层
        self.layer1 = Linear(in_features, hidden_features)
        self.layer2 = Linear(hidden_features, out_features)

    def forward(self, x):
        print("hello", x)
        x = self.layer1(x)
        print("world", x)
        x = t.sigmoid(x)
        return self.layer2(x)


perceptron = Perceptron(3, 4, 1)
for name, param in perceptron.named_parameters():
    print(name, param.size())

input1 = V(t.randn(2, 3))
output1 = perceptron.layer1(input1)
output2 = perceptron.layer2(output1)
print(output2)



/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
torch.Size([4, 3])
torch.Size([3])
tensor([[ 2.9238,  3.0967,  0.4950],
        [-0.5025, -0.0759, -0.9008]], grad_fn=<AddBackward0>)
w Parameter containing:
tensor([[-1.1720, -0.3929,  0.5265],
        [ 1.1065,  0.9273, -1.7421],
        [-0.7699,  0.7864, -1.9963],
        [ 0.5836,  1.0392,  0.8023]], requires_grad=True)
b Parameter containing:
tensor([0.5269, 0.5730, 0.1390], requires_grad=True)
Parameter containing:
tensor([[-1.1720, -0.3929,  0.5265],
        [ 1.1065,  0.9273, -1.7421],
        [-0.7699,  0.7864, -1.9963],
        [ 0.5836,  1.0392,  0.8023]], requires_grad=True)
Parameter containing:
tensor([0.5269, 0.5730, 0.1390], requires_grad=True)
torch.Size([3, 4])
torch.Size([4])
torch.Size([4, 1])
torch.Size([1])
layer1.w torch.Size([3, 4])
layer1.b torch.Size([4])
layer2.w torch.Size([4, 1])
layer2.b torch.Size([1])
tensor([[-2.2768],
        [-3.0801]], grad_fn=<AddBackward0>)

Process finished with exit code 0



多层感知机：

多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。
MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。 除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。

程序中的输入是：输入数据作为列向量是layer1(只有输出)，经过wx+b，得到layer2中激活前的值，在layer2中通过激活函数得到激活后的值，然后wx+b得到输出结果。



2. 常用的神经网络层

2.1 图像相关层

from PIL import Image
from torchvision.transforms import ToTensor,ToPILImage
from matplotlib import pyplot as plt
import torch as t
from torch import nn
from torch.autograd import Variable as V

to_tensor=ToTensor() # img->tensor
to_pil=ToPILImage()
lena=Image.open('..//imgs//lena.png')
plt.imshow(lena)
# plt.show()

# 输入一个batch，batch_size=1
input=to_tensor(lena).unsqueeze(0)
# print(input.shape)
# to_pil(input).show()
# to_pil(input.data.squeeze(0)).show()
input=input.view(1,1,-1,225)
print(input.shape)
# to_pil(input.data.squeeze(0)).show()


# 锐化卷积和
kernel=t.ones(3,3)/-9.
kernel[1][1]=1  # 卷积核中心点设置为1
conv=nn.Conv2d(1,1,(3,3),bias=False)
conv.weight.data=kernel.view(1,1,3,3)

out=conv(V(input))
print(out.data.squeeze(0).shape)
# to_pil(out.data.squeeze(0)).show()

pool=nn.AvgPool2d(2,2)
print(list(pool.parameters()))

out=out.unsqueeze(0)
out=pool(V(input))
to_pil(out.data.squeeze(0)).show()




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
torch.Size([1, 1, 675, 225])
torch.Size([1, 673, 223])
[]

Process finished with exit code 0








from torch.autograd import Variable as V
import torch as t
from torch import nn


# batch_size=4,维度是3
# t.manual_seed(0)
# input = V(t.randn(2,3))
input=V(t.arange(1.,7).view(2,3))
print(input)
linear=nn.Linear(3,4)
h=linear(input)
print("h=",h,h.shape)
# h=h.view(4,4)

# 4 channels   初始化标准差为4，均值为0

bn=nn.BatchNorm1d(4)
print("bn",bn)
bn.weight.data=t.ones(4)*4
print("bn.weight=",bn.weight.data)
bn.bias.data=t.zeros(4)
print(bn.bias.data)

bn_out=bn(h)
print("bn_out",bn_out)
print(bn_out.mean(0),bn_out.var(0,unbiased=False))


#  每个元素 以 0.5 的概率舍弃   有0.5的概率一半左右的数变为0
dropout = nn.Dropout(0.5)
o=dropout(bn_out)
print("o=",o)



/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[1., 2., 3.],
        [4., 5., 6.]])
h= tensor([[-0.4076,  0.3815,  2.3756, -0.8790],
        [ 0.0761,  1.2916,  4.8339, -2.6169]], grad_fn=<AddmmBackward>) torch.Size([2, 4])
bn BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
bn.weight= tensor([4., 4., 4., 4.])
tensor([0., 0., 0., 0.])
bn_out tensor([[-3.9997, -3.9999, -4.0000,  4.0000],
        [ 3.9997,  3.9999,  4.0000, -4.0000]],
       grad_fn=<NativeBatchNormBackward>)
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5763e-07],
       grad_fn=<MeanBackward1>) tensor([15.9973, 15.9992, 15.9999, 15.9998], grad_fn=<VarBackward1>)
o= tensor([[-7.9993, -0.0000, -0.0000,  7.9999],
        [ 0.0000,  7.9998,  8.0000, -7.9999]], grad_fn=<MulBackward0>)

Process finished with exit code 0









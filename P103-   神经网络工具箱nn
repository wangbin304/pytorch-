

1. nn.Module

torch.nn 是专门为深度学习设计的模块，核心数据结构是Module，它是一个抽象的概念，
既可以表示神经网络中的某个层(layer),也可以表示一个包含很多层的神经网络。
在实际应用中，最常见的做法是继承nn.Module，撰写自己的网络/层。

#  Module实现的全连接层，比利用Function实现的更简单，
#  因为它不需要写反向传播函数

import torch as t
from torch import nn
from torch.autograd import Variable as V

t.manual_seed(1000)


# 1. 自定义Linear 必须继承nn.Module
#  并且在构造函数中需要调用nn.Module的构造函数
#   super(Linear,self).__init__()
#   or   nn.Module.__init__(self)

# 2. 在构造函数__init__中必须自己定义可学习的参数，
#   并封装成Parameter(特殊 Variable) ，默认需要求导。

# 3. forward 前向传播，输入可以是多个Variable

# 4. 无需写反向传播函数，nn.Module利用autograd自动实现反向传播

# 5. 将layer可以看成数学概念中的函数，调用layer(input)即
#   可以得到对应结果，等价 layers.__call__(input)
#   在 __call__函数中，主要调用的是layer.forward(x)。

# 6. Module中的科学系参数通过 named_parameters()
#   或者parameters()返回迭代器，前者给parameter附上名字


class Linear(nn.Module):  # 继承 nn.Module
    def __init__(self, in_features, out_features):
        # 等价于 nn.Module.__init__(self)
        super(Linear, self).__init__()
        self.w = nn.Parameter(t.randn(in_features, out_features))
        print(self.w.shape)
        self.b = nn.Parameter(t.randn(out_features))
        print(self.b.shape)
        # # error: cannot resize variables that require grad
        # print(self.b.shape)
        # self.b.resize_(2,3)
        # print(self.b.size())

    def forward(self, x):
        x = x.mm(self.w)
        # print(self.b.expand_as(x).shape)
        # print(x.shape)
        return x + self.b.expand_as(x)


layer = Linear(4, 3)
input = V(t.randn(2, 4))
output = layer(input)
print(output)

#  named_parameters 参数有名字
for name, parameter in layer.named_parameters():
    print(name, parameter)  # w and b
#  parameters 参数没名字
for parameter in layer.parameters():
    print(parameter)  # w and b


class Perceptron(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        nn.Module.__init__(self)
        # super(Linear, self).__init__()
        # 此处的Linear是前面自定义的全连接层
        self.layer1 = Linear(in_features, hidden_features)
        self.layer2 = Linear(hidden_features, out_features)

    def forward(self, x):
        print("hello", x)
        x = self.layer1(x)
        print("world", x)
        x = t.sigmoid(x)
        return self.layer2(x)


perceptron = Perceptron(3, 4, 1)
for name, param in perceptron.named_parameters():
    print(name, param.size())

input1 = V(t.randn(2, 3))
output1 = perceptron.layer1(input1)
output2 = perceptron.layer2(output1)
print(output2)



/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
torch.Size([4, 3])
torch.Size([3])
tensor([[ 2.9238,  3.0967,  0.4950],
        [-0.5025, -0.0759, -0.9008]], grad_fn=<AddBackward0>)
w Parameter containing:
tensor([[-1.1720, -0.3929,  0.5265],
        [ 1.1065,  0.9273, -1.7421],
        [-0.7699,  0.7864, -1.9963],
        [ 0.5836,  1.0392,  0.8023]], requires_grad=True)
b Parameter containing:
tensor([0.5269, 0.5730, 0.1390], requires_grad=True)
Parameter containing:
tensor([[-1.1720, -0.3929,  0.5265],
        [ 1.1065,  0.9273, -1.7421],
        [-0.7699,  0.7864, -1.9963],
        [ 0.5836,  1.0392,  0.8023]], requires_grad=True)
Parameter containing:
tensor([0.5269, 0.5730, 0.1390], requires_grad=True)
torch.Size([3, 4])
torch.Size([4])
torch.Size([4, 1])
torch.Size([1])
layer1.w torch.Size([3, 4])
layer1.b torch.Size([4])
layer2.w torch.Size([4, 1])
layer2.b torch.Size([1])
tensor([[-2.2768],
        [-3.0801]], grad_fn=<AddBackward0>)

Process finished with exit code 0



多层感知机：

多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络，映射一组输入向量到一组输出向量。
MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。 除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。

程序中的输入是：输入数据作为列向量是layer1(只有输出)，经过wx+b，得到layer2中激活前的值，在layer2中通过激活函数得到激活后的值，然后wx+b得到输出结果。



2. 常用的神经网络层

2.1 图像相关层

from PIL import Image
from torchvision.transforms import ToTensor,ToPILImage
from matplotlib import pyplot as plt
import torch as t
from torch import nn
from torch.autograd import Variable as V

to_tensor=ToTensor() # img->tensor
to_pil=ToPILImage()
lena=Image.open('..//imgs//lena.png')
plt.imshow(lena)
# plt.show()

# 输入一个batch，batch_size=1
input=to_tensor(lena).unsqueeze(0)
# print(input.shape)
# to_pil(input).show()
# to_pil(input.data.squeeze(0)).show()
input=input.view(1,1,-1,225)
print(input.shape)
# to_pil(input.data.squeeze(0)).show()


# 锐化卷积和
kernel=t.ones(3,3)/-9.
kernel[1][1]=1  # 卷积核中心点设置为1
conv=nn.Conv2d(1,1,(3,3),bias=False)
conv.weight.data=kernel.view(1,1,3,3)

out=conv(V(input))
print(out.data.squeeze(0).shape)
# to_pil(out.data.squeeze(0)).show()

pool=nn.AvgPool2d(2,2)
print(list(pool.parameters()))

out=out.unsqueeze(0)
out=pool(V(input))
to_pil(out.data.squeeze(0)).show()




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
torch.Size([1, 1, 675, 225])
torch.Size([1, 673, 223])
[]

Process finished with exit code 0








from torch.autograd import Variable as V
import torch as t
from torch import nn


# batch_size=4,维度是3
# t.manual_seed(0)
# input = V(t.randn(2,3))
input=V(t.arange(1.,7).view(2,3))
print(input)
linear=nn.Linear(3,4)
h=linear(input)
print("h=",h,h.shape)
# h=h.view(4,4)

# 4 channels   初始化标准差为4，均值为0

bn=nn.BatchNorm1d(4)
print("bn",bn)
bn.weight.data=t.ones(4)*4
print("bn.weight=",bn.weight.data)
bn.bias.data=t.zeros(4)
print(bn.bias.data)

bn_out=bn(h)
print("bn_out",bn_out)
print(bn_out.mean(0),bn_out.var(0,unbiased=False))


#  每个元素 以 0.5 的概率舍弃   有0.5的概率一半左右的数变为0
dropout = nn.Dropout(0.5)
o=dropout(bn_out)
print("o=",o)



/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[1., 2., 3.],
        [4., 5., 6.]])
h= tensor([[-0.4076,  0.3815,  2.3756, -0.8790],
        [ 0.0761,  1.2916,  4.8339, -2.6169]], grad_fn=<AddmmBackward>) torch.Size([2, 4])
bn BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
bn.weight= tensor([4., 4., 4., 4.])
tensor([0., 0., 0., 0.])
bn_out tensor([[-3.9997, -3.9999, -4.0000,  4.0000],
        [ 3.9997,  3.9999,  4.0000, -4.0000]],
       grad_fn=<NativeBatchNormBackward>)
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5763e-07],
       grad_fn=<MeanBackward1>) tensor([15.9973, 15.9992, 15.9999, 15.9998], grad_fn=<VarBackward1>)
o= tensor([[-7.9993, -0.0000, -0.0000,  7.9999],
        [ 0.0000,  7.9998,  8.0000, -7.9999]], grad_fn=<MulBackward0>)

Process finished with exit code 0




2.2  激活函数


from torch import nn
import torch as t
from torch.autograd import Variable as V


t.manual_seed(1)
relu=nn.ReLU(inplace=True)
input=V(t.randn(2,3))
print(input)
output=relu(input)
print(output)
# .clamp 等价
# 通过 ReLU函数，input值变得和output一样，怀疑这俩一个空间
#  inplace 默认为 false，设置为true，输出将会直接覆盖到输入中
print(input)
print(input.clamp(max=0))


# Sequential 的三种写法
print("---Sequential---")
net1=nn.Sequential()
# net1.add_module("conv",nn.Conv2d(40,130,3))
net1.add_module("conv",nn.Conv2d(3,3,3))
net1.add_module("batchnorm",nn.BatchNorm2d(3))
net1.add_module("activation_layer",nn.ReLU())

net2=nn.Sequential(
    nn.Conv2d(3,3,3),
    nn.BatchNorm2d(3),
    nn.ReLU()
)

from collections import OrderedDict
net3=nn.Sequential(OrderedDict([("conv1",nn.Conv2d(3,3,3)),("bn1",nn.BatchNorm2d(3)),("relu1",nn.ReLU())]))
print("net1:",net1)
print("net2:",net2)
print("net3:",net3)

# 可以根据名字和序号取出子module
print(net1.conv,net2[0],net3.conv1)
print(net1.activation_layer,net2[2],net3.relu1)

# input=V(t.rand(1,40,4,4))
input=V(t.rand(1,3,4,4))
output=net1(input)
print(output)
output=net2(input)
output=net3(input)
output=net3.relu1(net1.batchnorm(net1.conv(input)))
# print("OK")
# print(input)
# print(net1.conv(input))
# print(net1.batchnorm(net1.conv(input)))
# print(output)


modelist=nn.ModuleList([nn.Linear(3,4),nn.ReLU(),nn.Linear(4,2)])
input=V(t.randn(1,3))
print(input)
for model in modelist:
    print("model---->\n",model)
    input=model(input)
    print("input---->\n",input)
# 下面会报错，因为modelist没有实现forward方法  in fact 并没有报错
output=modelist
print(output)
print(modelist)



class MyModule(nn.Module):
    def __init__(self):
        super(MyModule,self).__init__()
        self.list=[nn.Linear(3,4),nn.ReLU()]
        self.module_list=nn.ModuleList([nn.Conv2d(3,3,3),nn.ReLU()])

    def forward(self):
        pass
# list 中的子module并不能被主module识别，而ModuleList中的子module能
#够被主module识别。这意味着如果用list保存子module，将无法调整其参数，
# 因为其未能加入到主module的参数中
model=MyModule()
print("---MyModule---")
print(model)


for name,param in model.named_parameters():
    print(name,param.size())

model=MyModule().list
print(model)

# error
# for name,param in model.named_parameters():
#     print(name,param.size())






/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
tensor([[ 0.6614,  0.2669,  0.0617],
        [ 0.6213, -0.4519, -0.1661]])
tensor([[0.6614, 0.2669, 0.0617],
        [0.6213, 0.0000, 0.0000]])
tensor([[0.6614, 0.2669, 0.0617],
        [0.6213, 0.0000, 0.0000]])
tensor([[0., 0., 0.],
        [0., 0., 0.]])
---Sequential---
net1: Sequential(
  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (activation_layer): ReLU()
)
net2: Sequential(
  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU()
)
net3: Sequential(
  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu1): ReLU()
)
Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
ReLU() ReLU() ReLU()
tensor([[[[0.0000, 0.0000],
          [1.4725, 0.2500]],

         [[0.0000, 0.0000],
          [0.0000, 1.6477]],

         [[0.1241, 1.4152],
          [0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)
tensor([[ 2.5169,  0.1226, -0.3431]])
model---->
 Linear(in_features=3, out_features=4, bias=True)
input---->
 tensor([[-1.2820, -1.7654,  0.5785,  0.0805]], grad_fn=<AddmmBackward>)
model---->
 ReLU()
input---->
 tensor([[0.0000, 0.0000, 0.5785, 0.0805]], grad_fn=<ReluBackward0>)
model---->
 Linear(in_features=4, out_features=2, bias=True)
input---->
 tensor([[-0.1168, -0.2907]], grad_fn=<AddmmBackward>)
ModuleList(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): ReLU()
  (2): Linear(in_features=4, out_features=2, bias=True)
)
ModuleList(
  (0): Linear(in_features=3, out_features=4, bias=True)
  (1): ReLU()
  (2): Linear(in_features=4, out_features=2, bias=True)
)
---MyModule---
MyModule(
  (module_list): ModuleList(
    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
)
module_list.0.weight torch.Size([3, 3, 3, 3])
module_list.0.bias torch.Size([3])
[Linear(in_features=3, out_features=4, bias=True), ReLU()]

Process finished with exit code 0








2.3 循环神经网络






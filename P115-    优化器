
import torch as t
import torch.nn as nn
from torch import optim
from torch.autograd import Variable as V


class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.features=nn.Sequential(
            nn.Conv2d(3,6,5),
            nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(6,16,5),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
        )
        self.classifier=nn.Sequential(
            nn.Linear(16*5*5,120),
            nn.ReLU(),
            nn.Linear(120,84),
            nn.ReLU(),
            nn.Linear(84,10)
        )
    def forward(self,x):
            x=self.features(x)
            x=x.view(-1,16*5*5)
            x=self.classifier(x)
            return x
net=Net()

optimizer=optim.SGD(params=net.parameters(),lr=1)
optimizer.zero_grad()# 梯度清零

input=V(t.randn(1,3,32,32))
output=net(input)
output.backward(output) # fake backward

optimizer.step()  # 执行优化

# 为不同子网络设置不同的学习率，在finetune中经常用到

# 如果对某个参数不指定学习率，就是用默认学习率
optimizer=optim.SGD([{"params":net.features.parameters()},
                     {"params":net.classifier.parameters(),"lr":1e-2}
                     ],lr=1e-5)


# 只为两个全连接层设置较大的学习率，其余层的学习率较小

print([net.classifier[0],net.classifier[3]])

special_layers=nn.ModuleList([net.classifier[0],net.classifier[3]])

print(list(map(id,special_layers.parameters())))
print(map(id,special_layers.parameters()))

special_layers_params=list(map(id,special_layers.parameters()))

print(lambda p:id(p))

base_params=filter(lambda p:id(p) not in special_layers_params,net.parameters())

optimizer=t.optim.SGD([
    {"params":base_params},
    {"params":special_layers.parameters(),"lr":0.01}
],lr=0.001)




/home/wangbin/anaconda3/envs/deep_learning/bin/python3.7 /home/wangbin/anaconda3/envs/deep_learning/project/main.py
[Linear(in_features=400, out_features=120, bias=True), ReLU()]
[139633159409088, 139633159409168]
<map object at 0x7effb3b3bd90>
<function <lambda> at 0x7efee0de68c0>

Process finished with exit code 0
